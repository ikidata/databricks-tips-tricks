{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51042df2-9d15-415b-ba5e-1bfaa8d9b772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# REST API example for \"Bring your own data lineage\" for Databricks AI/BI Dashboards\n",
    "\n",
    "* Official documentation can be found here: https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/external-lineage\n",
    "* REST API documentation: https://docs.databricks.com/api/azure/workspace/externalmetadata/getexternalmetadata\n",
    "\n",
    "* Yes, lineage should be in the system tables, but you wonâ€™t find it in the visualized lineage yet ;D. That said, you can apply the same logic used here to other external systems.\n",
    "* Keep in mind that this quickly created notebook includes one-time insert logic only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "084cc7d8-103d-4c06-8bb2-fe8790974be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Things to consider:\n",
    "If you have multiple AI/BI dashboards, especially many that share the same tables, the data lineage can become messy when importing everything at once.\n",
    "\n",
    "Instead of applying mass automation, consider using this process selectively for only the most relevant dashboards. Currently, the tool scans all dashboards by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff22a4e5-8316-4d03-9a0c-702fe831c101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b65ac85-da74-4b9f-b5ac-6de037803f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from dbruntime.databricks_repl_context import get_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2404395b-7b8a-407e-a7ea-b498277aec48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get auth token & server hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbf5d60-3249-4a13-aa60-9f630d7539de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "server_hostname = f\"https://{get_context().browserHostName}\"\n",
    "token = get_context().apiToken\n",
    "current_user = get_context().user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3604c8fc-94be-4bb5-8f5e-25dab99bfbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ba6e44-da59-4e34-a7ef-12400456fc95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_rest_api(api_version: str, api_command: str, action_type: str,  payload: dict = {}) -> str:\n",
    "    '''\n",
    "    Dynamic rest api function. Since this is running in notebook, it's possible to use local server_hostname & token variables here. Keep in mind that I'm not breaking anything here, just returning error message if API call fails.\n",
    "    '''\n",
    "    try:\n",
    "        assert action_type in ['POST', 'GET'], f'Only POST and GET are supported but you used {action_type}'\n",
    "        url = f\"{server_hostname}/api/{api_version}/{api_command}\"\n",
    "        headers = {'Authorization': 'Bearer %s' % token}\n",
    "        session = requests.Session()\n",
    "\n",
    "        resp = session.request(action_type, url, data=json.dumps(payload), verify=True, headers=headers)\n",
    "        assert resp.status_code == 200, f\"Running REST API has failed with an error message: {resp.json()}\"\n",
    "        result = resp.json()\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def get_all_dashboards() -> list:\n",
    "    \"\"\"\n",
    "    Fetches all dashboards and returns a list of dictionaries,\n",
    "    where each dictionary maps dashboard_id to a cleaned display_name.\n",
    "    Cleaning:\n",
    "    - Lowercase\n",
    "    - Spaces to dashes\n",
    "    - Special characters removed\n",
    "    \"\"\"\n",
    "    \n",
    "    def text_cleaner(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', '-', text)\n",
    "        text = re.sub(r'[^a-z0-9\\-]', '', text)\n",
    "        text = re.sub(r'-{2,}', '-', text)\n",
    "        return text.strip('-')\n",
    "    \n",
    "    api_version = '2.0'\n",
    "    api_command = 'lakeview/dashboards'\n",
    "    action_type = 'GET'\n",
    "    \n",
    "    result = run_rest_api(api_version=api_version, api_command=api_command, action_type=action_type)\n",
    "\n",
    "    # Build list of dashboard_id: cleaned_display_name pairs\n",
    "    dashboard_list = [\n",
    "        {d['dashboard_id']: text_cleaner(d['display_name'])} \n",
    "        for d in result.get('dashboards', [])\n",
    "    ]\n",
    "\n",
    "    print(\"*\" * 25)\n",
    "    print(\"ALL AI/BI DASHBOARDS:\")\n",
    "    print(dashboard_list)\n",
    "    print(\"*\" * 25)\n",
    "\n",
    "    return dashboard_list\n",
    "\n",
    "\n",
    "def get_all_tables(dashboard_id: str) -> list:\n",
    "    '''\n",
    "    Get all tables from AI/BI Dashboard\n",
    "    '''\n",
    "    api_version = '2.0'\n",
    "    api_command = f'lakeview/dashboards/{dashboard_id}'\n",
    "    action_type = 'GET'\n",
    "    result = run_rest_api(api_version=api_version, api_command=api_command, action_type=action_type)['serialized_dashboard']\n",
    "\n",
    "    # Use Regex to find all tables (catalogName.schemaName.tableName)\n",
    "    pattern = r'\\b([\\w\\-]+)\\.([\\w\\-]+)\\.([\\w\\-]+)\\b'\n",
    "    matches = re.findall(pattern, result)\n",
    "\n",
    "    # Rebuild full table names from matches\n",
    "    tables = [f\"{catalog}.{schema}.{table}\" for catalog, schema, table in matches]\n",
    "\n",
    "    # Validate and keep only existing tables\n",
    "    validated_tables = []\n",
    "\n",
    "    for t in tables:\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE TABLE EXTENDED {t}\")\n",
    "            validated_tables.append(t)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    print(\"*\"*25)\n",
    "    print(f\"Found the next tables for Dashboard ID {dashboard_id}:\")\n",
    "    print(validated_tables)\n",
    "    print(\"*\"*25)\n",
    "    return validated_tables\n",
    "\n",
    "def get_all_columns_for_table(table_name: str) -> list:\n",
    "    column_list = spark.read.table(table_name).columns\n",
    "    print(\"-\"*25)\n",
    "    print(f\"Found the next columns for table {table_name}:\")\n",
    "    print(column_list)\n",
    "    print(\"-\"*25)\n",
    "    return column_list\n",
    "\n",
    "def create_external_metadata(dashboard_name: str, dashboard_id: str, column_list: list = []) -> None:\n",
    "    api_version = '2.0'\n",
    "    api_command = 'lineage-tracking/external-metadata'\n",
    "    action_type = 'POST'\n",
    "    payload = {\n",
    "  #      \"columns\": column_list,                         # <-- Activate this if you want to add columns as well\n",
    "        \"description\": \"Databricks AI/BI Dashboard\",\n",
    "        \"entity_type\": \"AI/BI Dashboard\",\n",
    "        \"name\": f\"{dashboard_name}\",\n",
    "        \"properties\": {\n",
    "                        \"custom_properties\": \"Ikidata\"\n",
    "                        },\n",
    "    #    \"owner\": current_user,  # Error message -> 'message': 'Must not supply an owner.' deactivated for purpose?\n",
    "        \"system_type\": \"OTHER\",  # DATABRICKS is mentioned in REST API documentation but isn't working - blocked? :(\n",
    "        \"url\": f\"{server_hostname}/dashboardsv3/{dashboard_id}\"\n",
    "    }\n",
    "    results = run_rest_api(api_version=api_version, api_command=api_command, action_type=action_type, payload=payload)\n",
    "    print(\"*\"*25)\n",
    "    print(f\"External metadata created successfully for dashboard '{dashboard_name}' ({dashboard_id})\")\n",
    "    print(\"*\"*25)\n",
    "\n",
    "def create_external_lineage(table_name: str, dashboard_name: str) -> str:\n",
    "    api_version = '2.0'\n",
    "    api_command = 'lineage-tracking/external-lineage'\n",
    "    action_type = 'POST'\n",
    "    payload = {\n",
    "        \"source\": {\n",
    "            \"table\": {\n",
    "                \"name\": table_name\n",
    "            }\n",
    "        },\n",
    "        \"target\": {\n",
    "            \"external_metadata\": {\n",
    "                \"name\": dashboard_name\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    result = run_rest_api(api_version=api_version, api_command=api_command, action_type=action_type, payload=payload)\n",
    "    print(\"*\"*25)\n",
    "    print(f\"Lineage added successfully for table {table_name} in dashboard '{dashboard_name}'\")\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f719873-ad8a-4a41-8bee-1e2c99c4ad34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get all AI/BI Dashboards. \n",
    "Consider limiting the scope here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebf23237-6761-4c2e-a621-196af2fe90ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dashboard_list = get_all_dashboards()                                      # Get all AI/BI Dashboards\n",
    "#dashboard_list = dashboard_list[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4392f49-a0f8-4073-9c99-98c5b3cadd28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loop all selected dashboards and let the automation handle the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3844a2c1-38e8-468a-9357-dbe285198c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for dashboard in dashboard_list:                                           # Loop all AI/BI Dashboards\n",
    "    dashboard_id = list(dashboard.keys())[0]                               # Get Dashboard ID\n",
    "    dashboard_name = list(dashboard.values())[0]                           # Get Dashboard name\n",
    "    table_list = get_all_tables(dashboard_id)                              # Get all tables Dashboard is using\n",
    "    create_external_metadata(dashboard_name, dashboard_id)                 # Create external metadata, columns deactivated\n",
    "    for table in table_list:                                               # Loop all tables\n",
    "        #column_list = get_all_columns_for_table(table)      # Get all columns for the table <- here you could loop all columns\n",
    "        create_external_lineage(table, dashboard_name)                     # Create external lineage for each table"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "external_data_lineage_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
